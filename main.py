# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mK7Tc0PuKV9vd53N3bbx1v8jqa3DTQci

# Step 0: Installing Necessary Libraries
"""

# Step 0: Installing Necessary Libraries
!pip install pymongo
!pip install pdfplumber

# Step 1: Importing Necessary Libraries
import requests  # For making HTTP requests
import os  # For file path manipulation
import json  # For handling JSON data
from pymongo import MongoClient  # For MongoDB interactions
import pdfplumber  # For PDF parsing
from collections import Counter  # For counting occurrences of words
import re  # For regular expressions
import time  # For time-related functions
from concurrent.futures import ThreadPoolExecutor  # For parallel processing
import tracemalloc  # For tracking memory usage

# Step 2: Loading the Dataset
file_path = 'C:/Users/Dell/Downloads/Dataset.json'  # Update with your local path
with open(file_path, 'r') as f:
    dataset = json.load(f)

print(dataset)  # Display the loaded dataset

# Step 3: Setup MongoDB Connection
client = MongoClient('mongodb://localhost:27017/')  # Connect to MongoDB
db = client['pdf_summarization']  # Use or create the 'pdf_summarization' database

# Step 4: Downloading and Saving PDFs
def download_pdf(url, file_name):
    response = requests.get(url)  # Make a request to download the PDF
    file_path = os.path.join(download_dir, file_name)  # Set the file path
    with open(file_path, 'wb') as file:
        file.write(response.content)  # Save the PDF content
    return file_path  # Return the path to the saved PDF

# Step 5: Parsing PDF and Extracting Text and Metadata
def parse_pdf(file_path):
    try:
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ''  # Extract text from each page

        # Metadata to be stored in MongoDB
        metadata = {
            'file_name': os.path.basename(file_path),  # Extract the file name
            'path': file_path,  # Store the file path
            'size': os.path.getsize(file_path),  # Store the file size
            'num_pages': len(pdf.pages)  # Store the number of pages
        }

        # Insert metadata into MongoDB
        doc_id = db.pdfs.insert_one(metadata).inserted_id
        return {'_id': doc_id, 'text': text, 'metadata': metadata}
    except Exception as e:
        print(f"Error processing {file_path}: {e}")  # Handle exceptions

# Step 6: Summarizing the Text
def summarize_text(text):
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)  # Split text into sentences
    word_count = Counter(text.split())  # Count word occurrences
    top_sentences = sorted(sentences, key=lambda x: sum([word_count[word] for word in x.split()]), reverse=True)  # Sort sentences by importance
    summary = " ".join(top_sentences[:5])  # Top 5 sentences for summary
    return summary

# Step 7: Function to Extract Keywords
def extract_keywords(text):
    words = re.findall(r'\b\w+\b', text.lower())  # Find all words
    stop_words = set(Counter(words).most_common(100))  # Get common stop words
    keywords = [word for word in words if word not in stop_words and len(word) > 3]  # Filter out stop words
    common_keywords = Counter(keywords).most_common(10)  # Get top 10 keywords
    return [keyword[0] for keyword in common_keywords]  # Return only the keywords

# Step 8: Processing a Single PDF
def process_single_pdf(i, url):
    file_name = f"pdf_{i + 1}.pdf"  # Create a file name for the PDF
    pdf_path = download_pdf(url, file_name)  # Download the PDF
    result = parse_pdf(pdf_path)  # Parse the PDF

    if result:
        print(f"\nExtracted Text from {file_name}:\n")
        print(result['text'])  # Display extracted text

        summary = summarize_text(result['text'])  # Summarize the text
        keywords = extract_keywords(result['text'])  # Extract keywords

        # Updating MongoDB with summary and keywords
        db.pdfs.update_one(
            {'_id': result['_id']},
            {'$set': {'summary': summary, 'keywords': keywords}}
        )
        print(f"Processed {file_name} and updated MongoDB.")

# Step 9: Loop Through Dataset and Process in Parallel
def process_dataset(dataset):
    start_time = time.time()  # Start timer
    tracemalloc.start()  # Start memory tracking

    with ThreadPoolExecutor() as executor:
        # Map the processing function to the dataset in parallel
        executor.map(lambda i_url: process_single_pdf(i_url[0], i_url[1]), enumerate(dataset.items()))

    end_time = time.time()  # End timer
    current, peak = tracemalloc.get_traced_memory()  # Get current and peak memory usage
    tracemalloc.stop()  # Stop memory tracking

    print(f"\nTotal processing time: {end_time - start_time:.2f} seconds")
    print(f"Current memory usage: {current / 10**6:.2f} MB; Peak was {peak / 10**6:.2f} MB")

# Step 10: Run the Dataset Processing
process_dataset(dataset)  # Execute the dataset processing