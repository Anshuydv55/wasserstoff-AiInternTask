# -*- coding: utf-8 -*-
"""main.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mK7Tc0PuKV9vd53N3bbx1v8jqa3DTQci
"""
!pip install pdfplumber
!pip install pymongo

import os
import json
import re
import requests
import pdfplumber
from pymongo import MongoClient
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
import tracemalloc
import time

# Constants
MONGODB_URI = 'mongodb://localhost:27017/'
DB_NAME = 'pdf_summarization'
DOWNLOAD_DIR = 'C:/Users/Dell/Downloads/pdfs/'  
TOP_SENTENCES_COUNT = 5
TOP_KEYWORDS_COUNT = 10

# Initialize MongoDB Client
client = MongoClient(MONGODB_URI)
db = client[pdf_summary]

class PDFProcessor:
    def __init__(self, dataset):
        self.dataset = dataset

    def download_pdf(self, url, file_name):
        """Downloads a PDF from a given URL and saves it locally."""
        response = requests.get(url)
        file_path = os.path.join(DOWNLOAD_DIR, file_name)
        with open(file_path, 'wb') as file:
            file.write(response.content)
        return file_path

    def parse_pdf(self, file_path):
        """Extracts text and metadata from a PDF file."""
        try:
            text = ""
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    text += page.extract_text() or ''
            metadata = {
                'file_name': os.path.basename(file_path),
                'path': file_path,
                'size': os.path.getsize(file_path),
                'num_pages': len(pdf.pages)
            }
            doc_id = db.pdfs.insert_one(metadata).inserted_id
            return {'_id': doc_id, 'text': text, 'metadata': metadata}
        except Exception as e:
            print(f"Error processing {file_path}: {e}")

    def summarize_text(self, text):
        """Generates a summary by selecting top sentences based on word frequency."""
        sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
        word_count = Counter(text.split())
        top_sentences = sorted(sentences, key=lambda x: sum(word_count[word] for word in x.split()), reverse=True)
        return " ".join(top_sentences[:TOP_SENTENCES_COUNT])

    def extract_keywords(self, text):
        """Extracts the most common keywords from the text."""
        words = re.findall(r'\b\w+\b', text.lower())
        stop_words = set(Counter(words).most_common(100))
        keywords = [word for word in words if word not in stop_words and len(word) > 3]
        return [keyword[0] for keyword in Counter(keywords).most_common(TOP_KEYWORDS_COUNT)]

    def process_single_pdf(self, i, url):
        """Processes a single PDF by downloading, parsing, and summarizing."""
        file_name = f"pdf_{i + 1}.pdf"
        pdf_path = self.download_pdf(url, file_name)
        result = self.parse_pdf(pdf_path)

        if result:
            print(f"\nExtracted Text from {file_name}:\n")
            print(result['text'])

            summary = self.summarize_text(result['text'])
            keywords = self.extract_keywords(result['text'])
            # Update MongoDB with summary and keywords
            db.pdfs.update_one(
                {'_id': result['_id']},
                {'$set': {'summary': summary, 'keywords': keywords}}
            )
            print(f"Processed {file_name} and updated MongoDB.")

    def process_dataset(self):
        """Processes the entire dataset in parallel."""
        start_time = time.time()
        tracemalloc.start()

        with ThreadPoolExecutor() as executor:
            executor.map(lambda i_url: self.process_single_pdf(i_url[0], i_url[1]), enumerate(self.dataset.items()))

        end_time = time.time()
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        print(f"\nTotal processing time: {end_time - start_time:.2f} seconds")
        print(f"Current memory usage: {current / 10**6:.2f}MB; Peak was {peak / 10**6:.2f}MB")

if __name__ == "__main__":
    # Step 2: Loading the Dataset
    file_path = 'C:/Users/Dell/Downloads/Dataset.json'  
    with open(file_path, 'r') as f:
        dataset = json.load(f)

    # Step 10: Run the Dataset Processing
    processor = PDFProcessor(dataset)
    processor.process_dataset()

